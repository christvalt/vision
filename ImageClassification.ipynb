{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.6"
    },
    "colab": {
      "name": "ImageClassification.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dLTkeMfY8fkU"
      },
      "source": [
        "# **Esercitazione su image classification**\n",
        "Nell'esercitazione odierna utilizzeremo le *Convolutional Neural Network* (CNN) per applicazioni di riconoscimento di volti (*Face Recognition*). \n",
        "\n",
        "Faremo uso del framework **TensorFlow**, sfruttando la libreria open-source **Keras** appositamente progettata per permettere una rapida prototipazione di reti neurali profonde.\n",
        "\n",
        "Alcuni link di approfondimento:\n",
        "- Introduzione a TensorFlow con utile schema grafico delle [API disponibili](https://ekababisong.org/gcp-ml-seminar/tensorflow/#navigating-through-the-tensorflow-api)\n",
        "- [Keras](https://keras.io/)\n",
        "\n",
        "Nello specifico potranno essere utilizzate due reti (VGG-16 e ResNet-50) pre-addestrate sui dataset [VGGFace](http://www.robots.ox.ac.uk/~vgg/data/vgg_face/) (contenente oltre 2 milioni di immagini di volti appartenenti a più di 2000 soggetti) e [VGGFace2](http://www.robots.ox.ac.uk/~vgg/data/vgg_face2/) (contenente oltre 3 milioni di immagini di volti appartenenti a più di 9000 soggetti).\n",
        "\n",
        "L'obiettivo dell'esercitazione è quello di utilizzare una CNN pre-addestrata come *feature extractor* per il riconoscimento di volti."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UlVcIij-B6z2"
      },
      "source": [
        "# **Operazioni preliminari**\n",
        "Prima di incominciare, è necessario eseguire alcune operazioni preliminari.\n",
        "\n",
        "Per ovviare a problemi di retrocompatibilità, il codice seguente aggiorna la versione di alcune librerie necessarie allo svolgimento dell'esercitazione. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ykW0DS9XSKLA"
      },
      "source": [
        "%tensorflow_version 1.x\n",
        "\n",
        "!pip uninstall -y h5py\n",
        "!pip install h5py==2.10.0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pp1JKhB7SK3W"
      },
      "source": [
        "Eseguendo la cella sottostante tutto il materiale necessario per lo svolgimento dell'esercitazione verrà scaricato sulla macchina remota. Alla fine dell'esecuzione selezionare il tab **Files** per verificare che tutto sia stato scaricato correttamente."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ay4bpiweCCDb"
      },
      "source": [
        "!wget http://bias.csr.unibo.it/VR/Esercitazioni/DBs/ImageClassification/FaceScrubSubset_Celebrities.zip\n",
        "!wget http://bias.csr.unibo.it/VR/Esercitazioni/PythonUtilities.zip\n",
        "\n",
        "!unzip /content/FaceScrubSubset_Celebrities.zip\n",
        "!unzip /content/PythonUtilities.zip\n",
        "\n",
        "!rm /content/FaceScrubSubset_Celebrities.zip\n",
        "!rm /content/PythonUtilities.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UC4EphfdTqiK"
      },
      "source": [
        "## **Installazione della libreria Keras-vggface**\n",
        "Per poter caricare velocemente i modelli preaddestrati sul dataset VGGFace2 si utilizzeranno alcune funzionalità della libreria [**Keras-vggface**](https://github.com/rcmalli/keras-vggface).\n",
        "\n",
        "Eseguire la cella sottostante per installare la libreria."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h6_zSOqbTrUK"
      },
      "source": [
        "!pip install git+https://github.com/rcmalli/keras-vggface.git\n",
        "!pip install keras_applications"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yAGlRsjXCCib"
      },
      "source": [
        "# **Import delle librerie**\n",
        "Per prima cosa è necessario eseguire l'import delle librerie utilizzate durante l'esecitazione."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hy9j5IO78fkV"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "import math\n",
        "from tensorflow import keras\n",
        "from keras.models import Model\n",
        "from keras_vggface.vggface import VGGFace\n",
        "\n",
        "import vr_utilities"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-4ogdD968fkZ"
      },
      "source": [
        "# **Dataset**\n",
        "Il dataset ultilizzato è composto da immagini RGB di volti di persone famose. In particolare utilizzeremo un sottoinsieme del [FaceScrub](http://vintage.winklerbros.net/facescrub.html) contenente 1590 immagini di 530 soggetti diversi (3  immagini per ciascuno di essi, 2 per il training e 1 per il test).\n",
        "\n",
        "Visto il numero esiguo di immagini (1060 per il dataset di training), cercare di addestrare da zero una CNN complessa (partendo da pesi random) risulta impossibile."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "ky7QZXOU8fka"
      },
      "source": [
        "db_path = '/content/Celebrities'\n",
        "train_filelist = 'TrainingSet.txt'\n",
        "test_filelist = 'TestSet.txt'\n",
        "labelnames_list = 'LabelNames.txt'\n",
        "\n",
        "print('Caricamento in corso ...')\n",
        "or_train_x, train_y = vr_utilities.load_labeled_dataset(train_filelist, db_path)\n",
        "or_test_x, test_y = vr_utilities.load_labeled_dataset(test_filelist, db_path)\n",
        "\n",
        "label_names = vr_utilities.load_label_names(labelnames_list, db_path)\n",
        "\n",
        "print('Shape training set:', or_train_x.shape)\n",
        "print('Shape test set:', or_test_x.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cc8AeObg8fke"
      },
      "source": [
        "La cella seguente contiene il codice per mostrare alcune immagini del training set. Guardando alcuni esempi si può facilmente notare la grande variabilità in termini di:\n",
        "- posa;\n",
        "- illuminazione;\n",
        "- espressione."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "bHVK45PQ8fkf"
      },
      "source": [
        "rows = 3\n",
        "columns = 6\n",
        "\n",
        "plt.rcParams.update({'font.size': 20})\n",
        "_, axs = plt.subplots(rows, columns, squeeze=False,figsize=(30, 15))\n",
        "samples = random.sample(range(len(label_names)), columns)\n",
        "\n",
        "for j in range(columns):\n",
        "    idx=samples[j]\n",
        "    sel_train_images=[or_train_x[k] for k in np.where(train_y==idx)[0]]\n",
        "    sel_test_images=[or_test_x[k] for k in np.where(test_y==idx)[0]]\n",
        "    sel_images=sel_train_images+sel_test_images\n",
        "    axs[0, j].set_title(label_names[idx])\n",
        "    for i in range(rows):\n",
        "        axs[i, j].axis('off')\n",
        "        axs[i, j].imshow(sel_images[i])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-59fss8s8fkj"
      },
      "source": [
        "# **Creazione del modello**\n",
        "Vediamo ora come creare un modello con il supporto di Keras e della libreria Keras-vggface. Utilizzare la variabile *model_name* per selezionare quale modello utilizza tra i due disponibili (VGG-16 e ResNet-50)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KbsDLAZPgeDt"
      },
      "source": [
        "model_name='vgg16'\n",
        "#model_name='resnet50'\n",
        "\n",
        "# create a vggface2 model\n",
        "model = VGGFace(model=model_name)\n",
        "\n",
        "print('Inputs: %s' % model.inputs)\n",
        "print('Outputs: %s' % model.outputs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ArcnpQjP8WN"
      },
      "source": [
        "## **Visualizzazione del modello**\n",
        "Eseguendo la cella seguente è possibile stampare un riepilogo testuale della struttura della rete."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZZivahXaPyma"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ps6qWnbeQBI9"
      },
      "source": [
        "Se si preferisce una visualizzazione grafica, eseguire la cella seguente."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ea1FXd8QQB6V"
      },
      "source": [
        "keras.utils.plot_model(model,show_shapes=True, show_layer_names=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MRdbWMTU2YIk"
      },
      "source": [
        "# **Creazione dell'estrattore di feature**\n",
        "L'addestramento di una CNN su un nuovo problema, richiede un\n",
        "training set etichettato di notevoli dimensioni .\n",
        "\n",
        "In alternativa al training da zero, possiamo utilizzare una rete esistente (pre-trained) per estrarre le feature generate ai livelli intermedi durante il passo forward ([*Transfer Learning*](https://cs231n.github.io/transfer-learning/)) e utilizzare queste feature per:\n",
        "1. addestrare un classificatore esterno (es. SVM) a\n",
        "riconoscere i pattern del nuovo dominio applicativo;\n",
        "2. stimare il grado di similarità tra feature estratte da immagini differenti utilizzando una metrica (es. distanza coseno).\n",
        "\n",
        "L'operazione di estrazione delle feature consiste nel calcolare, per ogni immagine fornita in input, l'output della rete al livello desiderato (*layer_name*).\n",
        "\n",
        "Per evitare, durante il passo *forward*, di attraversare livelli non necessari della rete si può creare una nuova istanza della classe [**Model**](https://keras.io/api/models/model/) il cui input sara il medesimo del modello originale mentre l'ouput sarà rappresentato dal livello da cui si vogliono estrarre le feature (*layer_name*)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yy_U3R3hiB3N"
      },
      "source": [
        "if (model_name=='vgg16'):\n",
        "  layer_name = 'fc7' #VGG-16\n",
        "elif (model_name=='resnet50'):\n",
        "  layer_name = 'avg_pool' #ResNet-50\n",
        "\n",
        "feature_extractor = Model(inputs=model.input,outputs=model.get_layer(layer_name).output)\n",
        "\n",
        "print('Inputs: %s' % feature_extractor.inputs)\n",
        "print('Outputs: %s' % feature_extractor.outputs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GkO3oqkG6vB8"
      },
      "source": [
        "## **Visualizzazione del nuovo modello**\n",
        "Eseguendo la cella seguente è possibile stampare un riepilogo testuale della struttura della rete da utilizzare come feature extractor."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-lpNZ35z7R4m"
      },
      "source": [
        "feature_extractor.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4sFFg8zC7Vvr"
      },
      "source": [
        "Se si preferisce una visualizzazione grafica, eseguire la cella seguente."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KloTr8Gp7X6_"
      },
      "source": [
        "keras.utils.plot_model(feature_extractor,show_shapes=True, show_layer_names=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5PMaxhcw8fkx"
      },
      "source": [
        "# ***Pre-processing* delle immagini**\n",
        "I modelli utilizzati sono stati addestrati con delle immagini pre-elaborate. Sarà necessario eseguire le medesime operazioni sia sul training che sul test set prima di poterli utilizzare."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jv0AGbYK5Ntg"
      },
      "source": [
        "## **Mapping**\n",
        "Se le immagini presentano delle intensità nel range [0;1] (come nel nostro caso), per prima cosa si dovrà \"mappare\" le intensità nel range [0;255]. Si esegua la cella seguente per effettuare il *mapping*. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X9V58CzZ4n7A"
      },
      "source": [
        "print('Range originale: [',np.min(or_train_x),';',np.max(or_train_x),']')\n",
        "\n",
        "norm_train_x=or_train_x*255\n",
        "norm_test_x=or_test_x*255\n",
        "\n",
        "print('Range ri-mappato: [',np.min(norm_train_x),';',np.max(norm_train_x),']')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N-uTn6NE34dw"
      },
      "source": [
        "## **Normalizzazione**\n",
        "Per rendere i modelli robusti rispetto a variazioni del contrasto e della luminosità, le immagini utilizzate per l'addestramento sono state preventivamente normalizzate (singolarmente) sottraendo a ogni pixel l'intensità media dell'intero dataset di training.\n",
        "\n",
        "Si esegua la cella seguente per normalizzare tutte le immagini sottraendovi l'intensità media del rispettivo training set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3RtJQcj0Frmy"
      },
      "source": [
        "if (model_name=='vgg16'):\n",
        "  mean_value=np.array([129.1863,104.7624,93.5940]) #RGB\n",
        "elif (model_name=='resnet50'):\n",
        "  mean_value=np.array([131.0912,103.8827,91.4953]) #RGB\n",
        "  \n",
        "print('Normalizzazione in corso ...')\n",
        "norm_train_x = norm_train_x-mean_value\n",
        "norm_test_x = norm_test_x-mean_value\n",
        "print('Normalizzazione completata')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u6m-2mW25RyM"
      },
      "source": [
        "## **Conversione RGB->BGR**\n",
        "Per ragioni \"storiche\", la maggior parte delle reti è stata addestrata con immagini il cui ordine dei canali è BGR e non RGB come ci si potrebbe aspettare. Pertanto, sarà prima necessario invertire l'ordine dei canali delle nostre immagini.\n",
        "\n",
        "Si esegua la cella successiva per invertire l'ordine dei canali da RGB a BGR."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zLw0MC5_8zYn"
      },
      "source": [
        "norm_train_x = norm_train_x[..., ::-1]\n",
        "norm_test_x = norm_test_x[..., ::-1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QvMRpZgk8fk1"
      },
      "source": [
        "# **Estrazione delle feature**\n",
        "Per estrarre le feature è sufficiente richiamare il metodo [**predict(...)**](https://keras.io/api/models/model_training_apis/#predict-method) del nostro estrattore (*feature_extractor*).\n",
        "\n",
        "Eseguire la cella seguente per estrarre le feature dal training e dal test set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NeuTWTQHZUOy"
      },
      "source": [
        "print('Estrazione delle feature...')\n",
        "train_features_x=feature_extractor.predict(norm_train_x)\n",
        "test_features_x=feature_extractor.predict(norm_test_x)\n",
        "\n",
        "print('Shape ndarray delle feature di train: ', train_features_x.shape)\n",
        "print('Shape ndarray delle feature di test: ', test_features_x.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x4wZYxMuUZ5T"
      },
      "source": [
        "Per comodità, può essere utile rimuovere le dimensioni unitarie tramite la funzione [**squeeze(...)**](https://numpy.org/doc/stable/reference/generated/numpy.squeeze.html) di NumPy."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VJEjiZxWUc-i"
      },
      "source": [
        "train_features_x=np.squeeze(train_features_x)\n",
        "test_features_x=np.squeeze(test_features_x)\n",
        "\n",
        "print('Shape ndarray delle feature di train: ', train_features_x.shape)\n",
        "print('Shape ndarray delle feature di test: ', test_features_x.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eZstXN7t8fk5"
      },
      "source": [
        "# **Face Recognition**\n",
        "Le feature appena estratte possono essere direttamente utilizzate insieme alla [distanza coseno](https://en.wikipedia.org/wiki/Cosine_similarity) per effettuare *face recognition* sulle nostre immagini.\n",
        "\n",
        "Dati due vettori **A** e **B**, la distanza coseno può essere calcolata come: \n",
        "\n",
        "\\begin{align}\n",
        "D_C(\\mathbf{A},\\mathbf{B})=1-\\frac{\\mathbf{A} \\cdot{} \\mathbf{B}}{\\lVert \\mathbf{A} \\rVert \\lVert \\mathbf{B} \\rVert}\n",
        "\\end{align}\n",
        "\n",
        "La funzione **compute_cosine_distances(...)**, definita nella cella seguente, calcola le distanze coseno delle feature di una immagine di test (*query_features_x*) da tutte le feature del training set (*train_features_x*). Questa implementazione permette di calcolare la norma di ogni immagine di test una sola volta. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2u-0EKpbqRck"
      },
      "source": [
        "def compute_cosine_distances(train_features_x,query_features_x):\n",
        "  cosine_distances=[]\n",
        "  norm_query=np.linalg.norm(query_features_x)\n",
        "  for train_feature in train_features_x:\n",
        "    norm_train=np.linalg.norm(train_feature)\n",
        "    cos_dist=1-np.dot(query_features_x, train_feature)/(norm_query*norm_train)\n",
        "    cosine_distances.append(cos_dist)\n",
        "\n",
        "  return np.asarray(cosine_distances)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sas_5lYi8fk9"
      },
      "source": [
        "## **Test**\n",
        "La cella sottostante calcola tutte le distanze coseno tra le feature del dataset di test e quelle di training memorizzandole nella variabile *test_distances*."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5HE-EkeiryiB"
      },
      "source": [
        "test_distances=[]\n",
        "print('Calcolo distanze coseno ...')\n",
        "for test_features in test_features_x:\n",
        "  test_distances.append(compute_cosine_distances(train_features_x,test_features))\n",
        "\n",
        "test_distances=np.asarray(test_distances)\n",
        "\n",
        "print('Shape ndarray delle distanze: ', test_distances.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L41bf0mJXr1w"
      },
      "source": [
        "È possibile misurare l'accuratezza del sistema di *face recognition* implementato eseguendo la cella successiva."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DY1OgW8bU2JP"
      },
      "source": [
        "test_distances_sorted_indices=np.argsort(test_distances,axis=1)\n",
        "\n",
        "predicted_y=train_y[test_distances_sorted_indices[:,0]]\n",
        "\n",
        "errors = predicted_y != test_y\n",
        "\n",
        "accuracy=1-(errors.sum()/len(errors))\n",
        "print('Accuracy sul test set: %.3f' % (accuracy))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a5VZ8aLt8flC"
      },
      "source": [
        "## **Visualizzazione errori**\n",
        "La cella seguente permette di visualizzare le immagini di test che vengono classificate in maniera errata. Sopra ad ogni immagine è riportato il nome del soggetto mentre a lato le classi più probabili."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jSftDCdgW6lC"
      },
      "source": [
        "error_indices = np.where(errors == True)[0]\n",
        "\n",
        "if error_indices.shape[0] > 0:\n",
        "  # Visualizzazione immagini\n",
        "  image_per_row = 2\n",
        "  top_class_count = 5\n",
        "\n",
        "  row_count=math.ceil(len(error_indices)/image_per_row)\n",
        "  column_count=image_per_row\n",
        "  plt.rcParams.update({'font.size': 12})\n",
        "  _, axs = plt.subplots(row_count, column_count,figsize=(20, 4*row_count),squeeze=False)\n",
        "    \n",
        "  for i in range(row_count):\n",
        "    for j in range(column_count):\n",
        "      axs[i,j].axis('off')\n",
        "\n",
        "  for i in range(len(error_indices)):\n",
        "    q = i // image_per_row\n",
        "    r = i % image_per_row\n",
        "    idx = error_indices[i]\n",
        "    \n",
        "    axs[q,r].imshow(or_test_x[idx])\n",
        "    axs[q,r].set_title(label_names[test_y[idx]])\n",
        "\n",
        "    best_indices=test_distances_sorted_indices[idx,0:2*top_class_count]\n",
        "    best_distances=test_distances[idx,best_indices]\n",
        "    \n",
        "    best_y = train_y[best_indices]\n",
        "    _, unique_indices = np.unique(best_y, return_index=True)\n",
        "    unique_indices=np.sort(unique_indices)\n",
        "    \n",
        "    text=''\n",
        "    for j in range(top_class_count):\n",
        "        text+='{}: {:.3f}\\n'.format(label_names[best_y[unique_indices[j]]],best_distances[unique_indices[j]])\n",
        "    \n",
        "    axs[q,r].text(330, 150, text, horizontalalignment='left', verticalalignment='center')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TWbGlEIM8flG"
      },
      "source": [
        "# **Esercizio**\n",
        "Utilizzare il sistema implementato per verificare a quale tra le celebrità presenti nel dataset assomigliate maggiormente.\n",
        "\n",
        "A tal fine:\n",
        "\n",
        "1. scattare una foto con il proprio volto in primo piano;\n",
        "2. ritagliarla per ottenere un'immagine quadrata (rapporto 1:1);\n",
        "3. riscalare l'immagine a una dimensione 224 x 224 pixel; \n",
        "4. trasferire l'immagine ottenuta su **Colab** utilizzando la funzione *Upload* del tab **Files**;\n",
        "5. caricare l'immagine in una variabile (per farlo può essere utile la funzione [**imread(...)**](https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.imread.html) della libreria [**Matplotlib**](https://matplotlib.org/));\n",
        "6. effettuare il *pre-processing* dell'immagine;\n",
        "7. estrarre le feature utilizzando l'estrattore creato;\n",
        "8. calcolare la distanza coseno tra le feature estratte e quelle del training set; \n",
        "9. visualizzare, utilizzando la libreria Matplotlib, il nome e le foto delle 3 celebrità più somiglianti."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TlzUz-VjcsV-"
      },
      "source": [
        "#..."
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}