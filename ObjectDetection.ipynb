{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.5.3"
    },
    "colab": {
      "name": "ObjectDetection.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8-_n4GkUB5f6"
      },
      "source": [
        "# **Esercitazione su object detection**\n",
        "Nell'esercitazione odierna utilizzeremo l'architettura SSD per rilevare gli oggetti nelle immagini del database [PASCAL VOC 2007](http://host.robots.ox.ac.uk/pascal/VOC/voc2007/).\n",
        "\n",
        "Faremo uso del framework **TensorFlow**, sfruttando la libreria open-source **Keras** appositamente progettata per permettere una rapida prototipazione di reti neurali profonde."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mEVVD4aeBmZc"
      },
      "source": [
        "# **Operazioni preliminari**\n",
        "Eseguendo la cella sottostante tutto il materiale necessario per lo svolgimento dell'esercitazione verrà scaricato sulla macchina remota. Alla fine dell'esecuzione selezionare il tab **Files** per verificare che tutto sia stato scaricato correttamente."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ajDToNkY_fJw"
      },
      "source": [
        "!wget http://host.robots.ox.ac.uk/pascal/VOC/voc2007/VOCtrainval_06-Nov-2007.tar\n",
        "!wget http://host.robots.ox.ac.uk/pascal/VOC/voc2007/VOCtest_06-Nov-2007.tar\n",
        "!wget http://bias.csr.unibo.it/VR/Esercitazioni/MaterialeEsObjectDetection.zip\n",
        "\n",
        "!tar -xvf /content/VOCtrainval_06-Nov-2007.tar\n",
        "!tar -xvf /content/VOCtest_06-Nov-2007.tar\n",
        "!unzip /content/MaterialeEsObjectDetection.zip\n",
        "\n",
        "!rm /content/VOCtrainval_06-Nov-2007.tar\n",
        "!rm /content/VOCtest_06-Nov-2007.tar\n",
        "!rm /content/MaterialeEsObjectDetection.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "546SZyys-agu"
      },
      "source": [
        "# **Import delle librerie**\n",
        "Per prima cosa è necessario eseguire l'import delle librerie utilizzate durante l'esecitazione."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tTTxQ1My-agx"
      },
      "source": [
        "from keras import backend as K\n",
        "from keras.models import load_model\n",
        "from keras.preprocessing import image\n",
        "import keras\n",
        "import tensorflow as tf\n",
        "from math import ceil\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "import cv2\n",
        "import random\n",
        "\n",
        "from models.keras_ssd300 import ssd_300\n",
        "from keras_loss_function.keras_ssd_loss import SSDLoss\n",
        "\n",
        "from ssd_encoder_decoder.ssd_input_encoder import SSDInputEncoder\n",
        "\n",
        "from data_generator.object_detection_2d_data_generator import DataGenerator\n",
        "from data_generator.object_detection_2d_geometric_ops import Resize\n",
        "from data_generator.object_detection_2d_photometric_ops import ConvertTo3Channels\n",
        "from data_generator.data_augmentation_chain_original_ssd import SSDDataAugmentation\n",
        "from data_generator.object_detection_2d_misc_utils import apply_inverse_transforms\n",
        "\n",
        "from misc.ssd_box_encode_decode_utils import decode_y\n",
        "\n",
        "%matplotlib inline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9wAW1X9QCBiT"
      },
      "source": [
        "# **Dataset per l'addestramento**\n",
        "In questa esercitazione useremo il dataset della competizione PASCAL VOC 2007.\n",
        "\n",
        "Il dataset consiste in un insieme di immagini RGB. Ogni immagine ha associato un file di annotazioni in formato XML (*ground truth*) contenente \n",
        "la classe e le bounding box relative a tutti gli oggetti presenti nell'immagine.\n",
        "\n",
        "Eseguendo la cella sottostante il dataset sarà caricato in memoria.\n",
        "\n",
        "Quando si usano database di grosse dimensioni non sempre è possibile e conveniente caricarli interamente in memoria. Per ovviare a questo problema è possibile utilizzare un *generator* che permette di gestire in maniera efficiente i dati caricandoli in memoria un batch alla volta. Generator più avanziati permettono anche di applicare ai dati tecniche di *data augmentation*. Nel nostro caso utilizzeremo un generator non tanto per questioni di memoria, ma per incrementare le dimensioni e la variabilità del training set tramite *data augmentation*."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-xw9AlK-CKaS"
      },
      "source": [
        "# Percorsi da cui caricare il database\n",
        "images_dir = '/content/VOCdevkit/VOC2007/JPEGImages/'\n",
        "annotations_dir = '/content/VOCdevkit/VOC2007/Annotations/'\n",
        "train_image_set_filename = '/content/VOCdevkit/VOC2007/ImageSets/Main/train.txt'\n",
        "val_image_set_filename = '/content/VOCdevkit/VOC2007/ImageSets/Main/val.txt'\n",
        "test_image_set_filename = '/content/VOCdevkit/VOC2007/ImageSets/Main/test.txt'\n",
        "\n",
        "# Classi degli oggetti presenti nel database\n",
        "classes = ['background',\n",
        "           'aeroplane', 'bicycle', 'bird', 'boat',\n",
        "           'bottle', 'bus', 'car', 'cat',\n",
        "           'chair', 'cow', 'diningtable', 'dog',\n",
        "           'horse', 'motorbike', 'person', 'pottedplant',\n",
        "           'sheep', 'sofa', 'train', 'tvmonitor']\n",
        "\n",
        "# Creazione di due DataGenerator: per il training e il validation set \n",
        "train_dataset = DataGenerator(load_images_into_memory=True)\n",
        "val_dataset = DataGenerator(load_images_into_memory=True)\n",
        "\n",
        "# Caricamento delle immagini e delle corrispondenti annotazioni di ground truth\n",
        "train_dataset.parse_xml(images_dirs=[images_dir],\n",
        "                        image_set_filenames=[train_image_set_filename],\n",
        "                        annotations_dirs=[annotations_dir],\n",
        "                        classes=classes)\n",
        "\n",
        "val_dataset.parse_xml(images_dirs=[images_dir],\n",
        "                      image_set_filenames=[val_image_set_filename],\n",
        "                      annotations_dirs=[annotations_dir],\n",
        "                      classes=classes)\n",
        "\n",
        "train_dataset_size = train_dataset.get_dataset_size()\n",
        "val_dataset_size   = val_dataset.get_dataset_size()\n",
        "\n",
        "print('Numero di immagini del training set:\\t{:>6}'.format(train_dataset_size))\n",
        "print('Numero di immagini del validation set:\\t{:>6}'.format(val_dataset_size))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9uEE4LDGCbJz"
      },
      "source": [
        "## **Visualizzazione dei dati**\n",
        "Per capire meglio la natura e la difficoltà del problema che affronteremo può essere molto utile visualizzare alcune immagini di esempio. Eseguendo la cella sottostante verranno visualizzate due immagini del training set scelte casualmente (con le rispettive annotazioni)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y8hNxPqLJzuG"
      },
      "source": [
        "colors = plt.cm.hsv(np.linspace(0, 1, len(classes)+1)).tolist()\n",
        "\n",
        "_, axs = plt.subplots(1, 2,figsize=(20, 10))\n",
        "for i in range(2):\n",
        "  random_idx=random.randint(0,train_dataset_size)\n",
        "  axs[i].imshow(train_dataset.images[random_idx]),axs[i].axis('off'),axs[i].set_title(train_dataset.image_ids[random_idx])\n",
        "  for box in train_dataset.labels[random_idx]:\n",
        "    xmin = box[1]\n",
        "    ymin = box[2]\n",
        "    xmax = box[3]\n",
        "    ymax = box[4]\n",
        "    color = colors[int(box[0])]\n",
        "    label = '{}'.format(classes[int(box[0])])\n",
        "    axs[i].add_patch(plt.Rectangle((xmin, ymin), xmax-xmin, ymax-ymin, color=color, fill=False, linewidth=2))  \n",
        "    axs[i].text(xmin, ymin, label, size='x-large', color='white', bbox={'facecolor':color, 'alpha':1.0})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Kj9F_lpEB-F"
      },
      "source": [
        "# **SSD**\n",
        "Il modello SSD (*Single Shot Detector*) è una rete in grado di individuare in tempo reale oggetti multipli all'interno di un'immagine con un elevato livello di accuratezza.\n",
        "\n",
        "![alt text](https://miro.medium.com/max/1948/1*51joMGlhxvftTxGtA4lA7Q.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AoQGPa7BX25g"
      },
      "source": [
        "## **Iperparametri**\n",
        "Nella cella sottostante sono riportati gli iperparametri più importanti per la creazione della rete."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X16iCWHW-ag2"
      },
      "source": [
        "img_height = 300 # Altezza dell'immagine di input del modello\n",
        "img_width = 300 # Larghezza dell'immagine di input del modello\n",
        "img_channels = 3 # Numero di canali dell'immagine di input del modello\n",
        "n_classes = 20 # Numero di classi del problema\n",
        "scales = [0.1, 0.2, 0.37, 0.54, 0.71, 0.88, 1.05] # Fattori di scala delle default box\n",
        "aspect_ratios = [[1.0, 2.0, 0.5], # Aspect ratio delle default box per ogni predictor layer\n",
        "                 [1.0, 2.0, 0.5, 3.0, 1.0/3.0],\n",
        "                 [1.0, 2.0, 0.5, 3.0, 1.0/3.0],\n",
        "                 [1.0, 2.0, 0.5, 3.0, 1.0/3.0],\n",
        "                 [1.0, 2.0, 0.5],\n",
        "                 [1.0, 2.0, 0.5]] \n",
        "steps = [8, 16, 32, 64, 100, 300] # Distanza tra default box adiacenti per ogni predictor layer\n",
        "offsets = [0.5, 0.5, 0.5, 0.5, 0.5, 0.5] # Offset del centro della prima default box rispetto al bordo dell'immagine riportato come frazione dello step per ogni prediction layer "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WePsr3HQ-ag6"
      },
      "source": [
        "## **Creazione del modello**\n",
        "\n",
        "Per istanziare un modello della SSD-300, utilizzando gli iperparametri appena impostati, è sufficiente richiamare la funzione **ssd_300(...)**. \n",
        "\n",
        "Con il metodo [**load_weights(...)**](https://keras.io/api/models/model_saving_apis/#loadweights-method) della classe **Model** è possibile caricare dei pesi del modello precedentemente salvati. Nel nostro caso verranno caricati i pesi della VGG-16 ottenuti addestrandola sulle immagini di ImageNet.\n",
        "\n",
        "La classe **SSDLoss** implementa la multi-task loss per SSD.\n",
        "\n",
        "Infine con il metodo **compile(...)** è possibile configurare *optimizer* e *loss* del modello.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fSdVUFCH-ag7"
      },
      "source": [
        "# File dei pesi iniziali del backbone VGG-16\n",
        "weights_path = '/content/weights/VGG_ILSVRC_16_layers_fc_reduced.h5'\n",
        "mean_color = [123, 117, 104] # Valore medio delle immagini utilizzate per addestrare la VGG-16\n",
        "\n",
        "# Modelli creati in precedenza vengono cancellati\n",
        "K.clear_session()\n",
        "\n",
        "model = ssd_300(image_size=(img_height, img_width, img_channels),\n",
        "                n_classes=n_classes,\n",
        "                mode='training',\n",
        "                scales=scales,\n",
        "                aspect_ratios_per_layer=aspect_ratios,\n",
        "                steps=steps,\n",
        "                offsets=offsets,\n",
        "                subtract_mean=mean_color)\n",
        "\n",
        "# Caricamento dei pesi iniziali della rete\n",
        "model.load_weights(weights_path, by_name=True)\n",
        "\n",
        "# Multi-task loss\n",
        "ssd_loss=SSDLoss()\n",
        "\n",
        "model.compile(optimizer='Adam', loss=ssd_loss.compute_loss)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QsWphpbpdXhb"
      },
      "source": [
        "### **Visualizzazione del modello**\n",
        "Eseguendo la cella seguente è possibile stampare un riepilogo testuale della struttura della rete."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hbWBNchQdaRG"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BQ_Ob19kdgck"
      },
      "source": [
        "Se si preferisce una visualizzazione grafica, eseguire la cella seguente."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dm1t0XMNdi7O"
      },
      "source": [
        "tf.keras.utils.plot_model(model,show_shapes=True, show_layer_names=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zT1O7pUe-ahC"
      },
      "source": [
        "# **Training**\n",
        "Ora siamo pronti per l'addestramento della SSD utilizzando le immagini del training set per un numero di epoche pari a *epoch_count* utilizzando *minibatch* di dimensione *batch_size*. Durante l'addestramento le prestazioni della rete verranno valutate anche sul validation set.\n",
        "\n",
        "La classe **SSDDataAugmentation** permette di applicare le trasformazioni di *data aumentation* utilizzate nell'addestramento di SSD.\n",
        "\n",
        "La classe **SSDInputEncoder** viene utilizzata per trasformare le annotazioni del ground truth (classe e bounding box) nel formato richiesto dal modello SSD."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T81WCwca-ahH"
      },
      "source": [
        "# Numero di epoche di addestramento\n",
        "epoch_count=1\n",
        "\n",
        "# Numero di pattern all'interno di ogni minibatch\n",
        "batch_size=32\n",
        "\n",
        "# Data augmentation per il training set\n",
        "ssd_data_augmentation = SSDDataAugmentation(img_height=img_height,\n",
        "                                            img_width=img_width,\n",
        "                                            background=mean_color)\n",
        "\n",
        "# Trasformazioni da applicare al validation set per renderlo conforme alle specifiche della rete\n",
        "convert_to_3_channels = ConvertTo3Channels()\n",
        "resize = Resize(height=img_height, width=img_width)\n",
        "\n",
        "# Per creare le default box è necessario specificare le dimensioni spaziali dei predictor layer\n",
        "predictor_sizes = [model.get_layer('conv4_3_norm_mbox_conf').output_shape[1:3],\n",
        "                  model.get_layer('fc7_mbox_conf').output_shape[1:3],\n",
        "                  model.get_layer('conv6_2_mbox_conf').output_shape[1:3],\n",
        "                  model.get_layer('conv7_2_mbox_conf').output_shape[1:3],\n",
        "                  model.get_layer('conv8_2_mbox_conf').output_shape[1:3],\n",
        "                  model.get_layer('conv9_2_mbox_conf').output_shape[1:3]]\n",
        "\n",
        "# Permette di trasformare le annotazionie del ground truth nel formato richiesto per addestrare l'SSD\n",
        "ssd_input_encoder = SSDInputEncoder(img_height=img_height,\n",
        "                                    img_width=img_width,\n",
        "                                    n_classes=n_classes,\n",
        "                                    predictor_sizes=predictor_sizes,\n",
        "                                    scales=scales,\n",
        "                                    aspect_ratios_per_layer=aspect_ratios,\n",
        "                                    steps=steps,\n",
        "                                    offsets=offsets,\n",
        "                                    matching_type='multi',\n",
        "                                    pos_iou_threshold=0.5,\n",
        "                                    neg_iou_limit=0.5)\n",
        "                                    \n",
        "# Creazione dei generator per il training e il validation set da passare al metodo fit\n",
        "train_generator = train_dataset.generate(batch_size=batch_size,\n",
        "                                         shuffle=True,\n",
        "                                         transformations=[ssd_data_augmentation],\n",
        "                                         label_encoder=ssd_input_encoder)\n",
        "\n",
        "val_generator = val_dataset.generate(batch_size=batch_size,\n",
        "                                     shuffle=False,\n",
        "                                     transformations=[convert_to_3_channels,resize],\n",
        "                                     label_encoder=ssd_input_encoder)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tJl9AyXgGp3e"
      },
      "source": [
        "Nella cella seguente viene richiamato il metodo [**fit(...)**](https://keras.io/api/models/model_training_apis/#fit-method) che esegue l'intera fase di addestramento in maniera automatica monitorando costantemente la loss function sul training e validation set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nHJbInkeHX3y"
      },
      "source": [
        "model.fit(x=train_generator,\n",
        "          validation_data=val_generator,\n",
        "          steps_per_epoch=ceil(train_dataset_size)/batch_size,\n",
        "          validation_steps=ceil(val_dataset_size)/batch_size,\n",
        "          epochs=epoch_count,\n",
        "          verbose=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F7PJVdqKRMXl"
      },
      "source": [
        "# **Caricamento di un modello preaddestrato**\n",
        "Come avrete notato l'addestramento di una rete di queste dimensioni può richiedere diverse ore.\n",
        "\n",
        "Pertanto, per proseguire l'esercitazione, creeremo un nuovo modello caricando i pesi dell'intera rete addestrata utilizzando training e validation set di PASCAL VOC 2007 e 2012.\n",
        "\n",
        "<u>Nota:</u> prima di eseguire la cella assicurarsi che gli iperparametri siano stati definiti nell'apposita sezione."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "thWn13GqRiOw"
      },
      "source": [
        "# File dei pesi dell'intera SSD-300 addestrata su Pascal VOC 2007+2012 con 120000 iterazioni\n",
        "weights_path = '/content/weights/VGG_VOC0712_SSD_300x300_iter_120000.h5'\n",
        "mean_color = [127.5,127.5,127.5] # Valore medio delle immagini utilizzate per addestrare l'intera SSD-300\n",
        "\n",
        "# Modelli creati in precedenza vengono cancellati\n",
        "K.clear_session()\n",
        "\n",
        "model = ssd_300(image_size=(img_height, img_width, img_channels),\n",
        "                n_classes=n_classes,\n",
        "                mode='inference',\n",
        "                scales=scales,\n",
        "                aspect_ratios_per_layer=aspect_ratios,\n",
        "                steps=steps,\n",
        "                offsets=offsets,\n",
        "                subtract_mean=mean_color)\n",
        "                \n",
        "# Caricamento dei pesi iniziali della rete\n",
        "model.load_weights(weights_path, by_name=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LvOPIlPzUKnV"
      },
      "source": [
        "# **Homework (facoltativo)**\n",
        "Provare a portare a termine l'addestramento utilizzando almeno 100 epoche e salvando il modello (o solamente i pesi ottenuti) con i metodi [**save(...)**](https://keras.io/api/models/model_saving_apis/#save-method) o [**save_weights(...)**](https://keras.io/api/models/model_saving_apis/#saveweights-method) variando i seguenti iperparametri:\n",
        "- il numero di epoche (*epoch_count*);\n",
        "- l'ottimizzatore (*optimizer*) utilizzato dalla rete per implementare la *back propagation*. Keras mette a disposizione vari [ottimizzatori](https://keras.io/api/optimizers/#available-optimizers);\n",
        "- i singoli iperparametri dell'ottimizzatore di cui il più importante è sicuramente il *learning rate*. Per una descrizione dettagliata dei vari parametri di ogni ottimizzatore fare riferimento alla documentazione di Keras."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HqpEuQcw-ahU"
      },
      "source": [
        "# **Funzioni per valutare le prestazioni**\n",
        "Una delle metriche più utilizzate per valutare l'accuratezza di un sistema di *object detection* è la [***medium Average Precision***](https://medium.com/@jonathan_hui/map-mean-average-precision-for-object-detection-45c121a31173) (mAP), calcolata come la media su tutte le classi del problema, dell'area sottesa alla curva *Recall/Precision* di ogni singola classe.\n",
        "\n",
        "Di seguito sono riportate le funzioni utilizzate per calcolare l'mAP:\n",
        "- **compute_overlap(...)** - calcola l'[*Intersection over Union*](https://en.wikipedia.org/wiki/Jaccard_index) (IoU) tra due bounding box;\n",
        "- **compute_ap(...)** - calcola l'*Average Precision* a partire dalla curva Recall/Precision;\n",
        "- **select_preds(...)** - seleziona le regioni più promettenti restituite dalla rete riscalando le bounding box alle dimensioni dell'immagine originale;\n",
        "- **extract_detections_and_annotations(...)** - i vari output della **detect(...)** vengono raggruppati rispetto alle classi degli oggetti effettivamente presenti nell'immagine (ground truth);\n",
        "- **compute_true_and_false_positives(...)** - per ogni classe calcola *True Positive* e *False Positive*;\n",
        "- **compute_class_average_precision(...)** - calcola l'*Average Precision* a partire da *True Positive* e *False Positive* sfruttando la **compute_ap(...)**;\n",
        "- **compute_medium_average_precision(...)** - calcola la mAP a partire dalle *Average Precision* di tutte le classi.\n",
        "\n",
        "Eseguire la cella sottostande per definire tutte queste funzioni."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nusw-L6QfoSq"
      },
      "source": [
        "def compute_overlap(a, b):\n",
        "    \n",
        "    #Code originally from https://github.com/rbgirshick/py-faster-rcnn.\n",
        "    #Parameters\n",
        "    #----------\n",
        "    #a: (N, 4) ndarray of float\n",
        "    #b: (K, 4) ndarray of float\n",
        "    #Returns\n",
        "    #-------\n",
        "    #overlaps: (N, K) ndarray of overlap between boxes and query_boxes\n",
        "    \n",
        "    area = (b[:, 2] - b[:, 0]) * (b[:, 3] - b[:, 1])\n",
        "\n",
        "    iw = np.minimum(np.expand_dims(a[:, 2], axis=1), b[:, 2]) - np.maximum(np.expand_dims(a[:, 0], 1), b[:, 0])\n",
        "    ih = np.minimum(np.expand_dims(a[:, 3], axis=1), b[:, 3]) - np.maximum(np.expand_dims(a[:, 1], 1), b[:, 1])\n",
        "\n",
        "    iw = np.maximum(iw, 0)\n",
        "    ih = np.maximum(ih, 0)\n",
        "\n",
        "    ua = np.expand_dims((a[:, 2] - a[:, 0]) * (a[:, 3] - a[:, 1]), axis=1) + area - iw * ih\n",
        "\n",
        "    ua = np.maximum(ua, np.finfo(float).eps)\n",
        "\n",
        "    intersection = iw * ih\n",
        "\n",
        "    return intersection / ua\n",
        "\n",
        "\n",
        "def compute_ap(recall, precision):\n",
        "    #Compute the average precision, given the recall and precision curves.\n",
        "    #Code originally from https://github.com/rbgirshick/py-faster-rcnn.\n",
        "    # Arguments\n",
        "    #    recall:    The recall curve (list).\n",
        "    #    precision: The precision curve (list).\n",
        "    # Returns\n",
        "    #    The average precision as computed in py-faster-rcnn.\n",
        "    \n",
        "    # correct AP calculation\n",
        "    # first append sentinel values at the end\n",
        "    mrec = np.concatenate(([0.], recall, [1.]))\n",
        "    mpre = np.concatenate(([0.], precision, [0.]))\n",
        "\n",
        "    # compute the precision envelope\n",
        "    for i in range(mpre.size - 1, 0, -1):\n",
        "        mpre[i - 1] = np.maximum(mpre[i - 1], mpre[i])\n",
        "\n",
        "    # to calculate area under PR curve, look for points\n",
        "    # where X axis (recall) changes value\n",
        "    i = np.where(mrec[1:] != mrec[:-1])[0]\n",
        "\n",
        "    # and sum (\\Delta recall) * prec\n",
        "    ap = np.sum((mrec[i + 1] - mrec[i]) * mpre[i + 1])\n",
        "    return ap\n",
        "\n",
        "\n",
        "def select_preds(y_pred,output_image_shape,confidence_threshold=0.5):\n",
        "  y_pred_thresh = [y_pred[k][y_pred[k,:,1] > confidence_threshold] for k in range(y_pred.shape[0])]\n",
        "\n",
        "  pred_boxes = []\n",
        "  pred_labels = []\n",
        "  for box in y_pred_thresh[0]:\n",
        "    xmin = box[2] * output_image_shape[1] / img_width\n",
        "    ymin = box[3] * output_image_shape[0] / img_height\n",
        "    xmax = box[4] * output_image_shape[1] / img_width\n",
        "    ymax = box[5] * output_image_shape[0] / img_height\n",
        "    class_id = int(box[0])\n",
        "    score = box[1]\n",
        "\n",
        "    pred_boxes.append([xmin, ymin, xmax, ymax, score])\n",
        "    pred_labels.append(class_id)\n",
        "\n",
        "  return np.array(pred_boxes),np.array(pred_labels)\n",
        "\n",
        "\n",
        "def extract_detections_and_annotations(pred_boxes,pred_labels,true_labels):\n",
        "  detections = [None for i in range(len(classes))]\n",
        "  annotations = [None for i in range(len(classes))]\n",
        "\n",
        "  l = range(1, len(classes))\n",
        "  for label in l:\n",
        "      if(len(pred_labels)):\n",
        "          detections[label] = pred_boxes[pred_labels == label, :]\n",
        "\n",
        "  for label in l:\n",
        "      if len(true_labels) > 0:\n",
        "          annotations[label] = true_labels[true_labels[:, 0] == label, 1:5].copy()\n",
        "      else:\n",
        "          annotations[label] = np.array([[]])\n",
        "\n",
        "  return detections,annotations\n",
        "\n",
        "\n",
        "def compute_true_and_false_positives(all_detections,all_annotations,label,conf_threshold=0.5):\n",
        "  false_positives = np.zeros((0,))\n",
        "  true_positives = np.zeros((0,))\n",
        "  scores = np.zeros((0,))\n",
        "  num_annotations = 0.0\n",
        "\n",
        "  for i in range(len(all_detections)):\n",
        "      annotations = all_annotations[i][label]\n",
        "      annotations = annotations.astype(np.float32)\n",
        "\n",
        "      num_annotations += annotations.shape[0]\n",
        "      detected_annotations = []\n",
        "      detections = all_detections[i][label]\n",
        "      if(detections is not None):\n",
        "          detections = detections.astype(np.float32)\n",
        "\n",
        "          for d in detections:\n",
        "              scores = np.append(scores, d[4])\n",
        "\n",
        "              try:\n",
        "                  annotations[0][0]\n",
        "              except IndexError:\n",
        "                  false_positives = np.append(false_positives, 1)\n",
        "                  true_positives = np.append(true_positives, 0)\n",
        "                  continue\n",
        "\n",
        "              overlaps = compute_overlap(np.expand_dims(d, axis=0), annotations)\n",
        "              assigned_annotation = np.argmax(overlaps, axis=1)\n",
        "              max_overlap = overlaps[0, assigned_annotation]\n",
        "              \n",
        "              if max_overlap >= conf_threshold and assigned_annotation not in detected_annotations:\n",
        "              \n",
        "                  false_positives = np.append(false_positives, 0)\n",
        "                  true_positives = np.append(true_positives, 1)\n",
        "                  detected_annotations.append(assigned_annotation)\n",
        "              else:\n",
        "                  false_positives = np.append(false_positives, 1)\n",
        "                  true_positives = np.append(true_positives, 0)\n",
        "\n",
        "  return true_positives,false_positives,scores,num_annotations\n",
        "\n",
        "\n",
        "def compute_class_average_precision(true_positives,false_positives,scores,num_annotations):\n",
        "  if num_annotations == 0:\n",
        "      return 0\n",
        "  \n",
        "  indices = np.argsort(-scores)\n",
        "  false_positives = false_positives[indices]\n",
        "  true_positives = true_positives[indices]\n",
        "\n",
        "  false_positives = np.cumsum(false_positives)\n",
        "  true_positives = np.cumsum(true_positives)\n",
        "\n",
        "  recall = true_positives / num_annotations\n",
        "  \n",
        "  precision = true_positives / np.maximum(true_positives + false_positives, np.finfo(np.float64).eps)\n",
        "\n",
        "  return compute_ap(recall, precision)\n",
        "\n",
        "\n",
        "def compute_medium_average_precision(average_precisions):\n",
        "  count = 0\n",
        "  for k in average_precisions.keys():\n",
        "    count  = count + float(average_precisions[k])\n",
        "\n",
        "  return count/len(range(1, len(classes)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7qeewGLffosR"
      },
      "source": [
        "# **Esempio di detection sul validation set**\n",
        "Per eseguire la *detection* utilizzare il metodo **predict(...)** passandogli in input le immagini da analizzare (dopo averle riscalate alle dimensioni di input della rete).\n",
        "\n",
        "Sfruttando le funzioni definite in precedenza è possibile selezionare le regioni più promettenti.\n",
        "\n",
        "Il parametro *confidence_threshold* viene utilizzato per selezionare le regioni finali dall'insieme di bounding box restituite dalla rete.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4jrMp8w1CN9l"
      },
      "source": [
        "confidence_threshold = 0.5 # Valore di confidenza utilizzato per la selezione delle regioni\n",
        "show_gt = False # Flag per la visualizzazione del ground truth\n",
        "val_image_count = 4 # Numero di immagini da valutare\n",
        "\n",
        "# Selezione casuale di val_image_count immagini del validation set e loro ridimensionamento\n",
        "random_indices=[]\n",
        "input_images=[]\n",
        "for i in range(val_image_count):\n",
        "  random_idx=random.randint(0,val_dataset_size)\n",
        "  resized_image=cv2.resize(val_dataset.images[random_idx],(img_height,img_width))\n",
        "  random_indices.append(random_idx)\n",
        "  input_images.append(resized_image)\n",
        "\n",
        "# Detection\n",
        "y_preds = model.predict(np.array(input_images))\n",
        "print('Shape del volume di output: ',y_preds.shape)\n",
        "\n",
        "all_detections = [None for i in range(val_image_count)]\n",
        "all_annotations = [None for i in range(val_image_count)]\n",
        "\n",
        "colors = plt.cm.hsv(np.linspace(0, 1, len(classes)+1)).tolist()\n",
        "_, axs = plt.subplots(1, val_image_count,figsize=(20, 10))\n",
        "for i in range(val_image_count):\n",
        "  # Selezione delle regioni più promettenti\n",
        "  pred_boxes,pred_labels=select_preds(y_preds[i][np.newaxis,:,:],val_dataset.images[random_indices[i]].shape,confidence_threshold)\n",
        "\n",
        "  true_labels = np.array(val_dataset.labels[random_indices[i]])\n",
        "  detections,annotations=extract_detections_and_annotations(pred_boxes,pred_labels,true_labels)\n",
        "\n",
        "  all_detections[i]=detections\n",
        "  all_annotations[i]=annotations\n",
        "\n",
        "  # Visualizzazione dell'immagine\n",
        "  axs[i].imshow(val_dataset.images[random_indices[i]]),axs[i].axis('off'),axs[i].set_title(val_dataset.image_ids[random_indices[i]])\n",
        "\n",
        "  # Visualizzazione del ground truth\n",
        "  if show_gt:\n",
        "    for box in val_dataset.labels[random_indices[i]]:\n",
        "      xmin = box[1]\n",
        "      ymin = box[2]\n",
        "      xmax = box[3]\n",
        "      ymax = box[4]\n",
        "      color = colors[int(box[0])]\n",
        "      label = '{}'.format(classes[int(box[0])])\n",
        "      axs[i].add_patch(plt.Rectangle((xmin, ymin), xmax-xmin, ymax-ymin, color='green', fill=False, linewidth=2))  \n",
        "      axs[i].text(xmin, ymin, label, size='x-large', color='white', bbox={'facecolor':'green', 'alpha':1.0})\n",
        "\n",
        "  # Visualizzazione degli oggetti individuati\n",
        "  for j in range(pred_boxes.shape[0]):\n",
        "    box=pred_boxes[j]\n",
        "    label=pred_labels[j]\n",
        "    xmin = box[0]\n",
        "    ymin = box[1]\n",
        "    xmax = box[2]\n",
        "    ymax = box[3]\n",
        "    color = colors[int(label)]\n",
        "    label = '{}: {:.2f}'.format(classes[int(label)], box[4])\n",
        "    axs[i].add_patch(plt.Rectangle((xmin, ymin), xmax-xmin, ymax-ymin, color=color, fill=False, linewidth=2))  \n",
        "    axs[i].text(xmin, ymin, label, size='x-large', color='white', bbox={'facecolor':color, 'alpha':1.0})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vMbqVeqBrxo0"
      },
      "source": [
        "## **Valutazione delle prestazioni**\n",
        "Eseguendo la cella seguente verrà calcolata l'*Average Precision* per ogni classe e l'mAP sui risultati ottenuti nella cella precedente."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oGoWCTsjr4D0"
      },
      "source": [
        "average_precisions = {}\n",
        "\n",
        "# Calcolo dell'Average Precision per ogni classe del problema\n",
        "for label in range(1, len(classes)):\n",
        "  true_positives,false_positives,scores,num_annotations=compute_true_and_false_positives(all_detections,all_annotations,label)\n",
        "  ap=compute_class_average_precision(true_positives,false_positives,scores,num_annotations)\n",
        "  average_precisions[classes[label]]=ap\n",
        "\n",
        "print(average_precisions)\n",
        "print('mAP: ',compute_medium_average_precision(average_precisions))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BNDc_7ywOJmr"
      },
      "source": [
        "# **Valutazione delle prestazioni sul test set**\n",
        "Misurare le prestazioni sul dataset di test per verificarne l'effettiva capacità di generalizzazione."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x7qUyYMUOV5L"
      },
      "source": [
        "## **Test set**\n",
        "Eseguendo la cella sottostante il dataset di test sarà caricato in memoria."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u2RY-4XcOYCj"
      },
      "source": [
        "# Creazione del DataGenerator\n",
        "test_dataset = DataGenerator(load_images_into_memory=True)\n",
        "\n",
        "# Caricamento delle immagini e delle corrispondenti annotazioni di ground truth\n",
        "test_dataset.parse_xml(images_dirs=[images_dir],\n",
        "                      image_set_filenames=[test_image_set_filename],\n",
        "                      annotations_dirs=[annotations_dir],\n",
        "                      classes=classes)\n",
        "\n",
        "test_dataset_size = test_dataset.get_dataset_size()\n",
        "\n",
        "print('Numero di immagini del test set:\\t{:>6}'.format(test_dataset_size))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N_wClg3NcGIi"
      },
      "source": [
        "### **Visualizzazione dei dati**\n",
        "Eseguendo la cella sottostante verranno visualizzate due immagini del test set scelte casualmente (con le rispettive annotazioni)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ThkWlTVfcNiY"
      },
      "source": [
        "colors = plt.cm.hsv(np.linspace(0, 1, len(classes)+1)).tolist()\n",
        "\n",
        "_, axs = plt.subplots(1, 2,figsize=(20, 10))\n",
        "for i in range(2):\n",
        "  random_idx=random.randint(0,test_dataset_size)\n",
        "  axs[i].imshow(test_dataset.images[random_idx]),axs[i].axis('off'),axs[i].set_title(test_dataset.image_ids[random_idx])\n",
        "  for box in test_dataset.labels[random_idx]:\n",
        "    xmin = box[1]\n",
        "    ymin = box[2]\n",
        "    xmax = box[3]\n",
        "    ymax = box[4]\n",
        "    color = colors[int(box[0])]\n",
        "    label = '{}'.format(classes[int(box[0])])\n",
        "    axs[i].add_patch(plt.Rectangle((xmin, ymin), xmax-xmin, ymax-ymin, color=color, fill=False, linewidth=2))  \n",
        "    axs[i].text(xmin, ymin, label, size='x-large', color='white', bbox={'facecolor':color, 'alpha':1.0})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HlGeiCEbbsj6"
      },
      "source": [
        "## **Object detection e calcolo delle prestazioni**\n",
        "Eseguendo la cella sottostante si effettuerà la *detection* sulle immagini del test set utilizzando il modello attuale (*model*).\n",
        "\n",
        "Per ogni immagine analizzata verrà riportata l'*Average Precision* per ogni classe.\n",
        "\n",
        "Alla fine dell'esecuzione sarà riportato il valore di mAP.\n",
        "\n",
        "<u>Nota:</u> visto l'alto numero di immagini contenute nel test set, inizialmente si consiglia di eseguire la *detection* solo su un sottoinsieme (impostando opportunamente la variabile *test_image_count*)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "afj9ijSgJFh-"
      },
      "source": [
        "confidence_threshold=0.5 # Valore di confidenza utilizzato per la selezione delle regioni\n",
        "test_image_count=200 # Numero di immagini del test set da valutare\n",
        "\n",
        "all_detections = [None for i in range(test_image_count)]\n",
        "all_annotations = [None for i in range(test_image_count)]\n",
        "for i in range(test_image_count):\n",
        "    print(test_dataset.image_ids[i], end = '')\n",
        "        \n",
        "    # Riscalatura dell'immagine alle dimensioni di input della rete\n",
        "    resized_image=cv2.resize(test_dataset.images[i],(img_height,img_width))\n",
        "\n",
        "    # Detection\n",
        "    y_pred = model.predict(resized_image[np.newaxis,:,:,:])\n",
        "\n",
        "    # Selezione delle regioni più promettenti\n",
        "    pred_boxes,pred_labels=select_preds(y_pred,test_dataset.images[i].shape,confidence_threshold)\n",
        "\n",
        "    true_labels = np.array(test_dataset.labels[i])\n",
        "    detections,annotations=extract_detections_and_annotations(pred_boxes,pred_labels,true_labels)\n",
        "    \n",
        "    all_detections[i]=detections\n",
        "    all_annotations[i]=annotations\n",
        "\n",
        "    true_label_aps={}\n",
        "    for label in true_labels:\n",
        "      true_positives,false_positives,scores,num_annotations=compute_true_and_false_positives([detections],[annotations],label[0])\n",
        "      ap=compute_class_average_precision(true_positives,false_positives,scores,num_annotations)\n",
        "      true_label_aps[classes[label[0]]]=ap\n",
        "\n",
        "    print('\\t',true_label_aps)\n",
        "\n",
        "# Calcolo dell'Average Precision per ogni classe del problema\n",
        "average_precisions = {}\n",
        "for label in range(1, len(classes)):\n",
        "  true_positives,false_positives,scores,num_annotations=compute_true_and_false_positives(all_detections,all_annotations,label)\n",
        "  ap=compute_class_average_precision(true_positives,false_positives,scores,num_annotations)\n",
        "  average_precisions[classes[label]]=ap\n",
        "\n",
        "print(average_precisions)\n",
        "print('mAP: ',compute_medium_average_precision(average_precisions))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rV1iGrRuVdZb"
      },
      "source": [
        "### **Selezione della soglia di confidenza ottimale**\n",
        "Verificare l'influenza della soglia di confidenza sulle prestazioni della rete variando il parametro *confidence_threshold* e rieseguendo la cella precedente. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BiY4u9PRT4dI"
      },
      "source": [
        "## **Visualizzare il risultato su singola immagine**\n",
        "Eseguendo la cella sottostante sarà possibile visualizzare il risultato della *detection* su una specifica immagine del test set (impostando opportunamente la variabile *image_id*)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "POVpVN_LL-6G"
      },
      "source": [
        "confidence_threshold = 0.5 # Valore di confidenza utilizzato per la selezione delle regioni\n",
        "show_gt = True # Flag per la visualizzazione del ground truth\n",
        "image_id ='000001' # ID dell'immagine del test set da processare\n",
        "\n",
        "image_idx=test_dataset.image_ids.index(image_id)\n",
        "\n",
        "# Riscalatura dell'immagine alle dimensioni di input della rete\n",
        "resized_image=cv2.resize(test_dataset.images[image_idx],(img_height,img_width))\n",
        "\n",
        "# Detection\n",
        "y_pred = model.predict(resized_image[np.newaxis,:,:,:])\n",
        "\n",
        "# Selezione delle regioni più promettenti\n",
        "pred_boxes,pred_labels=select_preds(y_pred,test_dataset.images[image_idx].shape,confidence_threshold)\n",
        "\n",
        "# Visualizzazione dell'immagine\n",
        "_, ax = plt.subplots(figsize=(10,10))\n",
        "ax.imshow(test_dataset.images[image_idx]),ax.axis('off')\n",
        "\n",
        "# Visualizzazione del ground truth\n",
        "if show_gt:\n",
        "  for box in test_dataset.labels[image_idx]:\n",
        "    xmin = box[1]\n",
        "    ymin = box[2]\n",
        "    xmax = box[3]\n",
        "    ymax = box[4]\n",
        "    color = colors[int(box[0])]\n",
        "    label = '{}'.format(classes[int(box[0])])\n",
        "    ax.add_patch(plt.Rectangle((xmin, ymin), xmax-xmin, ymax-ymin, color='green', fill=False, linewidth=2))  \n",
        "    ax.text(xmin, ymin, label, size='x-large', color='white', bbox={'facecolor':'green', 'alpha':1.0})\n",
        "\n",
        "# Visualizzazione degli oggetti individuati\n",
        "for j in range(pred_boxes.shape[0]):\n",
        "  box=pred_boxes[j]\n",
        "  label=pred_labels[j]\n",
        "  xmin = box[0]\n",
        "  ymin = box[1]\n",
        "  xmax = box[2]\n",
        "  ymax = box[3]\n",
        "  color = colors[int(label)]\n",
        "  label = '{}: {:.2f}'.format(classes[int(label)], box[4])\n",
        "  ax.add_patch(plt.Rectangle((xmin, ymin), xmax-xmin, ymax-ymin, color=color, fill=False, linewidth=2))  \n",
        "  ax.text(xmin, ymin, label, size='x-large', color='white', bbox={'facecolor':color, 'alpha':1.0})"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}